# Chapter5 backpropagation

신경망의 가중치 매개변수 기울기 구하는 방법
1) 수치미분 - 단순하고 구현하기 쉽지만 계산시간이 오래 걸림
2) 오차역전파법

오차역전파법 이해하기. 두가지 방법 모두 활용하자.
1) 수식을 통해 이해 - 정확, 간결, 올바름
2) 그래프를 통해 이해 - 시각적 이해 도움

## 5.1 computational graph
* 계산 그래프 : 계산 과정을 그래프로 나타냄 - 그래프 자료구조
* 순전파(forward propagation) : 계산을 왼쪽에서 오른쪽으로 진행함
* 역전파(backword propatation) : 계산을 오른쪽에서 왼쪽으로 반대방향으로 진행함

### 5.1.3 계산 그래프르 이용하는 이유
1. 복잡한 문제를 단순화할 수 있다.
2. 중간 계산 결과를 모두 보관할 수 있다.
3. 역전파를 통해 미분값을 효율적으로 계산할 수 있다.

## 5.2 연쇄법칙 chain rule
역전파는 국소적인 미분을 오른쪽에서 왼쪽으로 전달.

### 5.2.2 연쇄법칙이란?
연쇄법칙 : 합성함수의 미분은 합성함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.

## 5.3 역전파
### 5.3.1 덧셈 노드의 역전파
덧셈은 미분하면 1이므로 입력 값을 그대로 흘려보낸다.

### 5.3.2 곱셈 노드의 역전파
각 미분값을 곱한
![곱셈 노드의 역전파](fig%205-12.png)다

## 5.4 단순한 계층 구현하기
* 다음 절에서는 신경망을 구성하는 '계층' 각각을 하나의 클래스로 구현함. 계층이란 신경망의 기능 단위를 의미함.

### 5.4.1 곱셈 계층 , 5.4.2 덧셈 계층
simple_layer.py

## 5.5 활성화 함수 계층 구현하기
### 5.5.1 ReLU
ReLU 계층은 전기 회로의 스위치에 비유할 수 있다.<br>
순전파 때 전류가 흐르고 있으면 스위치 ON, 그렇지 않으면 OFF. 역전파때 ON이면 흐르고, OFF면 흐르지 않음.

### 5.5.2 시그모이드
시그모이드 계층의 역전파는 순전파의 출력만으로도 계산할 수 있다.
![시그모이드 계층의 역전파](e%205.12.png)
노드를 그룹화하여 간소화하면 중간과정 생략, 세세한 내용을 노출하지 않고 입력과 출력에만 집중 가능.

## 5.6 Affine/Softmax 계층 구현하기
### 5.6.1 Affine 계층, 5.6.2 배치용 Affine 계층
### 5.6.3 softmax-with-loss layer
affine_softmax_layer.py

신경망에서 수행하는 작업 : 학습, 추론<br>
추론할 때에는 일반적으로 softmax layer를 사용하지 않는다.<br> 
신경망 추론에서 답을 하나만 내는 경우에는 불필요하기 때문. 학습할때는 필요하다.<br>

소프트맥스의 손실 함수로 '교차 엔트로피 오차'를 사용하면 역전파가 말끔하게 떨어진다.
'오차제곱합'도 말끔하게 떨어지므로 회귀의 출력층에서 '항등 함수'의 손실함수로 '오차제곱합'을 이용하면 말끔함.

# 5.7 오차역전파법 구현하기
계층을 사용하여 인식 결과를 얻는 처리(predict)와 기울기를 구하는 처리(gradient) 계층의 전파만으로 동작이 이뤄짐.

two_layer_net.py

* 순서가 있는 딕셔너리 사용하여 순전파, 역전파
* 수치 미분은 오차역전파를 정확하게 구현했는지 확인하기 위해 필요하다. 두 방식으로 도출한 값이 일치하는지 확인하는 작업을 해야한다(=gradient check)

# 5.8 정리
* 계산 그래프를 활용한 순전파, 역전파 이해 및 활용
* 각 신경망 계층의 구현
* 오차역전파법을 적용한 신경망 구현
* 오차역전파법으로 구한 값 검층 (수치미분값과 비교)