# Chapter4 신경망 학습
* 신경망 학습 : 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득함<br>
* 손실 함수 : 신경망이 학습할 수 있도록 해주는 지표<br>
* 학습의 목표 : 손실 함수의 결과값을 가장 작게 만드는 가중치 매개변수를 찾기<br>
* 경사법 : 함수의 기울기를 활용하여 손실 함수의 값을 가급적 작게 만드는 기법중 하나

## 4.1 데이터에서 학습한다!
가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다는 의미.

cf) 퍼셉트론 수렴 정리(perceptron convergence theorem) : 선형 분리 가능 문제라면 퍼셉트론도 자동으로 학습 가능하다. (비선형은 불가.)

### 4.1.1 데이터 주도 학습
* 데이터
* 특징 feature : 입력 데이터에서 본질적인 데이터를 정확하게 추출할 수 있도록 설계된 변환기

방법1 : 처음부터 규칙을 사람이 파악하여 만들기.<br>
방법2 : 알고리즘을 설계하는 대신 주어진 데이터를 잘 활용해서 해결해보기 = 특징을 추출하고 그 패턴을 기계학습 기술로 학습하기

기계학습에서는 모아진 데이터로부터 규칙을 찾아내는 역할은 기계가 해주지만, 이때 사용하는 feature는 사람이 설계한다.<br>
사람이 적절한 특징을 사용하도록 해야하며 결과에 영향을 미친다.

방법3 : 딥러닝. 특징까지 사람이 설계하지 않고 기계가 스스로 학습하게 한다. 종단간 기계학습(end-to-end machine learning)

### 4.1.2 훈련 데이터와 시험 데이터
모델의 범용 능력을 제대로 평가하기 위해 훈련 데이터와 시험 데이터의 분리 필요.
* 훈련 데이터 training data : 학습을 위한 데이터. 이걸 이용해서 최적의 매개변수를 찾기.
* 시험 데이터 test data : 훈련된 모델의 실력 평가

cf) overfitting : 한 데이터셋에만 지나치게 최적화된 상태. 이걸 피하는 것이 기계학습의 중요한 과제중 하나.

## 4.2 손실 함수
loss function. 최적의 매개변수 값을 찾기 위한 지표.

일반적으로는 오차제곱합과 교차 엔트로피 오차를 사용한다. 임의의 함수를 사용할 수도 있음.

### 4.2.1 오차제곱합
sum of squares for error, SSE
![오차제곱합](e%204.1.png)

### 4.2.2 교차 엔트로피 오차
cross entropy error, CEE
![교차엔트로피오차](e%204.2.png)

### 4.2.3 미니배치 학습
학습 데이터는 여러 셋이 있으므로, 각 데이터셋마다 오차를 구해 평균을 내야한다. (=평균 손실 함수 구하기)<br>
그런데 훈련 데이터가 매우 많다면 일일이 손실 함수를 계산하는것은 현실적이지 않다.

현실적인 방법 : 데이터의 일부를 추려 전체의 근사치로 이용하도록 한다.<br>
신경망 학습에서도 훈련 데이터로부터 일부만 골라 학습을 수행한다. = 미니배치 mini-batch

### 4.2.4, 4.2.5
손실 함수를 설정하는 이유?

매개변수의 값을 변화시켰을 때 손실함수가 어떻게 변하는지(=미분값)에 따라 값을 조절해야하는데<br>
정확도를 지표로 삼게 되맨 매개변수의 미분이 대부분의 장소에서 0이 되기 때문에 매개변수의 갱신이 되지 않는다.

활성화 함수의 미분값이 중요한 이유도 여기서 나온다.<br>
미분값이 0이 아니고 어느정도로 연속적으로 변하는가?가 학습에 영향을 미친다.

## 4.3 수치 미분
미분 개념 정리. 소스코드만 정리.

## 4.4 기울기
* 중요 : 기울기가 가리키는 쪽은 각 포인트에서 함수의 출력 값을 가장 크게 줄이는 방향이다.

### 4.4.1 경사법 gradient method
미션 : 손실 함수가 최솟값이 될 때의 매개변수 값(=최적값)을 찾아내기.

이걸 기울기를 잘 이용해서 찾아보려는 방법이 경사법.

기울기가 가리키는 곳에 정말 최적값이 있는지는 보장하지 않는다. 이것을 단서로 나아갈 방향을 정하는 것.

경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동하여 경사를 구하고, 다시 이동하기를 반복

* 경사 하강법 gradient descent method : 최솟값 찾기
* 경사 상승법 gradient ascent method : 최댓값 찾기 (부호를 반전시키면 하강과 동일함)

#### 학습률 learning rate
한번의 학습으로 얼만큼 학습해야 할지 정하는 값. 매개변수의 값을 얼마나 갱신하느냐를 정한다.<br>
미리 특정 값으로 정해두어야 하고, 너무 크거나 작지 않도록 조절해야한다.

이렇게 자동으로 얻어지는 매개변수가 아니라 사람이 직접 설정해야 하는 매개변수를 "하이퍼파라미터 hyper parameter", "초매개변수"라고 한다.

## 4.5 학습 알고리즘 구현하기
[학습절차]
* 전제 : 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라고 한다.
* 1단계 - 미니배치 : 훈련데이터 랜덤 선택으로 미니배치 구성. [확률적 경사 하강법 stochastic gradient descent] 미니배치의 손실함수 값을 줄이는 것을 목표로 수행할 예정
* 2단계 - 기울기 산출 : 각 가중치 매개변수의 기울기를 구하여 손실함수 값을 줄일 수 있는 방향 정하기
* 3단계 - 매개변수 갱신 : 2단계에서의 기울기를 참고로 매개변수 갱신
* 4단계 - 반복 : 원하는만큼으로 손실함수 값이 줄어들때까지 반복

에폭 epoch : 학습데이터로 학습할 수 있는 횟수 단위. 10000개 데이터를 100개 미니배치로 학습하면 100회 반복하면 데이터 소진됨. 100회가 1에폭임.

## 4.6 정리
* 신경망 학습의 목표 : 손실함수(지표)를 기준으로 값이 가장 작아지는 가중치 매개변수 값 찾기
* 경사법
* 훈련데이터 / 시험데이터를 나누는 이유와 범용 능력
* 수치 미분